# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00-core.ipynb (unless otherwise specified).

__all__ = ['flatten_dict', 'stack_dicts', 'add_suffix', 'pad_to_size', 'logprobs_from_logits', 'whiten',
           'clip_by_value', 'entropy_from_logits', 'average_torch_dicts', 'stats_to_np', 'build_bert_batch_from_txt',
           'add_attr', 'counter_pad_tokens']

# Cell
import torch
import torch.nn.functional as F
import collections
import numpy as np

# Cell

def flatten_dict(nested, sep='/'):
    """Flatten dictionary and concatenate nested keys with separa tor."""
    def rec(nest, prefix, into):
        for k, v in nest.items():
            if sep in k:
                raise ValueError(f"separator '{sep}' not allowed to be in key '{k}'")
            if isinstance(v, collections.Mapping):
                rec(v, prefix + k + sep, into)
            else:
                into[prefix + k] = v
    flat = {}
    rec(nested, '', flat)
    return flat

def stack_dicts(stats_dicts):
    """Stack the values of a dict."""
    results = dict()
    for k in stats_dicts[0]:
        stats_list = [torch.flatten(d[k]) for d in stats_dicts]
        results[k] = torch.stack(stats_list)
    return results

def add_suffix(input_dict, suffix):
    """Add suffix to dict keys."""
    return dict((k + suffix, v) for k,v in input_dict.items())

# Cell

def pad_to_size(tensor, size, dim=1, padding=50256):
    """Pad tensor to size."""
    t_size = tensor.size()[dim]
    if t_size==size:
        return tensor
    else:
        return torch.nn.functional.pad(tensor, (0,size-t_size), 'constant', padding)

def logprobs_from_logits(logits, labels):
    """
    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591
    """
    logp = F.log_softmax(logits, dim=2)
    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logpy,logp


def whiten(values, shift_mean=True):
    """Whiten values."""
    mean, var = torch.mean(values), torch.var(values)
    whitened = (values - mean) * torch.rsqrt(var + 1e-8)
    if not shift_mean:
        whitened += mean
    return whitened

def clip_by_value(x, tensor_min, tensor_max):
    """
    Tensor extenstion to torch.clamp
    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713
    """
    clipped = torch.max(torch.min(x, tensor_max), tensor_min)
    return clipped

def entropy_from_logits(logits):
    """Calculate entropy from logits."""
    pd = torch.nn.functional.softmax(logits, dim=-1)
    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd*logits, axis=-1)
    return entropy


def average_torch_dicts(list_of_dicts):
    """Average values of a list of dicts wiht torch tensors."""
    average_dict = dict()
    for key in list_of_dicts[0].keys():
        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)
    return average_dict

def stats_to_np(stats_dict):
    """Cast all torch.tensors in dict to numpy arrays."""
    new_dict = dict()
    for k, v in stats_dict.items():
        if isinstance(v, torch.Tensor):
            new_dict[k] = v.detach().cpu().numpy()
        else:
            new_dict[k] = v
        if np.isscalar(new_dict[k]):
            new_dict[k] = float(new_dict[k])
    return new_dict


# Cell

def build_bert_batch_from_txt(text_list, tokenizer, device):
    """Create token id and attention mask tensors from text list for BERT classification."""

    # tokenize
    tensors = [tokenizer.encode(txt, return_tensors="pt").to(device) for txt in text_list]

    # find max length to pad to
    max_len = max([t.size()[1] for t in tensors])

    # get padded tensors and attention masks
    # (attention masks make bert ignore padding)
    padded_tensors = []
    attention_masks = []
    for tensor in tensors:
        attention_mask = torch.ones(tensor.size(), device=device)
        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))
        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))

    # stack all tensors
    padded_tensors = torch.cat(padded_tensors)
    attention_masks = torch.cat(attention_masks)

    return padded_tensors, attention_masks

# Cell

def add_attr(obj, attr_dict):
    for k,v in attr_dict.items():
        setattr(obj, k,v)


# Cell

def counter_pad_tokens(sentences_ids, pad_token_id=0):
    '''
    args:
        - sentences_ids: 2D matrix[batch size, longest sentence length].
        - pad_token_id: the pad token id, default 0 in T5 tokenizer.
    return the lart non pad token of every sentence, shape [batch size]
    '''
    if len(sentences_ids.size()) == 2:
        end_token_indexs = []
        for sentence_id in sentences_ids:
            if sentence_id.is_cuda:
                temp = sentence_id.cpu().numpy()
            else:
                temp = sentence_id.numpy()
            idx = (temp[::-1]!=pad_token_id).argmax(axis=0)
            end_token_indexs.append(idx)
        return torch.tensor(end_token_indexs)
    elif len(sentences_ids.size()) == 1:
        if sentences_ids.is_cuda:
            temp = sentences_ids.cpu().numpy()
        else:
            temp = sentences_ids.numpy()
            idx = (temp[::-1]!=pad_token_id).argmax(axis=0)
            return idx
    else:
        return -1


def find_end_tokens_idxs(sentences_ids, pad_token_id=0, end_token_id=1):
    """
    args:
        - sentences_ids: 2D matrix[batch size, longest sentence length].
        - pad_token_id: the pad token id, default 0 in T5 tokenizer.
        - end_token_id: the end token id, default 1 in T5 tokenizer.
    return the end token idx in the generation.
    """
    sentence_len = sentences_ids.shape[-1]
    end_token_indexs = []
    for sentence_id in sentences_ids:
        end_idx = [ i for i,id in enumerate(sentence_id) if id == end_token_id]
        if len(end_idx) != 0 :
            end_idx = end_idx[0] - 1 # due to the one step deviation
        else:
            end_idx = sentence_len - 1
        # assert temp[end_idx] == end_token_id
        end_token_indexs.append(end_idx)
    return torch.tensor(end_token_indexs)
